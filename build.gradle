description = "A research library for graph processing"
group = "software.uncharted"
version = "0.1-SNAPSHOT"

// Project-level shared variables
ext {
  dependencyScalaVersion = "2.11"
  scalaVersion = "2.11.8"
  sparkVersion = "2.0.1"
  hbaseVersion = "1.0.0-cdh5.5.2"
}

// Extra setup needed for plugins
buildscript {
  repositories {
    mavenCentral()
    jcenter()
  }

  dependencies {
    classpath "com.github.maiflai:gradle-scalatest:0.10"
    classpath 'org.scoverage:gradle-scoverage:1.0.9'
    classpath "org.github.ngbinh.scalastyle:gradle-scalastyle-plugin_2.11:0.9.0"
  }
}

apply plugin: 'scala'
apply plugin: 'maven'
apply plugin: 'maven-publish'
apply plugin: 'distribution'
apply plugin: 'idea'
apply plugin: 'com.github.maiflai.scalatest'
apply plugin: 'scalaStyle'


// Gradle/intellij don't work properly together with 1.8
sourceCompatibility = '1.7'

// maven repositories
repositories {
  mavenCentral()
  mavenLocal()
  maven {
    url = "https://repository.cloudera.com/artifactory/cloudera-repos"
  }
  maven {
    url = "http://maven.oculus.local:8080/archiva/repository/snapshots"
  }
  maven {
    url = "http://maven.oculus.local:8080/archiva/repository/internal"
  }
}

// Task to update the gradle wrapper
task wrapper(type: Wrapper) {
  gradleVersion = '2.13'
}

// Test and scoverage tests should be forced to single threaded execution
// since Spark can't run multiple contexts within in a single JVM.  We ignore
// tests relying on external systems (HBase + S3) by default, but they can be
// include by adding -PincludeExternalTests to the command line.  Running tests on
// a closed build environment like Bamboo block all IO, so an option is provided
// to block those via -PexcludeIoTests.
test {
  maxParallelForks = 1
  jvmArgs += ["-XX:MaxPermSize=512m", "-Xmx1000m"]
  if (project.hasProperty('excludeIoTests')) {
    tags {
      exclude 's3.test'
      exclude 'hbc.test'
      exclude 'fileio.test'
    }
  } else if (!project.hasProperty('includeExternalTests')) {
    tags {
      exclude 's3.test'
      exclude 'hbc.test'
    }
  }

}

// configure scala style checking plugin
scalaStyle {
  configLocation = "scalastyle_config.xml"
  includeTestSourceDirectory = true
  source = sourceSets.main.allScala
  testSource = sourceSets.test.allScala
  failOnWarning = false
  verbose = true
}

// Configure a jar task to build a fat jar, run it before the assembly step
task assemblyJar(type: Jar) {
  classifier = "assembly"
  from files(sourceSets.main.output.classesDir)
  from files(sourceSets.main.output.resourcesDir)

  from {
    (configurations.runtime - configurations.provided + configurations.forceInclude).collect {
      it.isDirectory() ? it : zipTree(it)
    }
  }
}
assemblyJar.mustRunAfter "jar"
assemble.dependsOn "assemblyJar"

// extra configurations
configurations {
  provided
  compile.extendsFrom provided
  dist
  forceInclude
  // IntelliJ is picking up an old version of jetty that we need to
  // force exclude to run tests.
  provided.exclude group: "org.jboss.netty", module: "netty"
}
// Add scripts zip to the artifacts of the dist assembly.  This can be used by downstream builds.
artifacts {
  dist assemblyJar
}

// Common dependency functions
ext {
  // A function to exclude various HBase dependencies we don't want.
  hbaseDependency = { dependencyName, config = "compile" ->
    getDependencies().add(config, "org.apache.hbase:" + dependencyName + ":$hbaseVersion", {
      exclude group: "asm", module: "asm"
      exclude group: "org.slf4j", module: "slf4j-api"
      exclude group: "org.slf4j", module: "slf4j-log4j12"
      exclude group: "org.mortbay.jetty"
      exclude group: "org.jboss.netty", module: "netty"
      exclude group: "io.netty", module: "netty"
      exclude group: "org.apache.hadoop", module: "hadoop-core"
    })
  }

  // A function that defines a project dependency if the useProjects option is set, otherwise fetches
  // from maven.
  optionalProjectDependency = { dependencyName ->
    boolean projectExists = false
      depends = getDependencies()
      projectName = ":" + depends.create(dependencyName).getName()

      for (p in project.getAllprojects()) {
        if (":"+p.getName() == projectName) projectExists = true
      }

    if (projectExists && useProjects && useProjects == 'true') {
      println("Depending on "+dependencyName+" directly")
        dependencies {
          compile project(projectName)
        }
    } else {
      println("Depending on "+dependencyName+" through Maven")
        dependencies {
          compile dependencyName
        }
    }
  }
}

// Jars / projects this project depends on.
dependencies {
  // dependencies that can be taken from a maven repo or local projects
  optionalProjectDependency "software.uncharted.salt:salt-core:4.0.3"
  optionalProjectDependency "software.uncharted.sparkpipe:sparkpipe-core:0.9.8"
  optionalProjectDependency "software.uncharted.xdata:xdata-salt-tiling:0.1.2"

  // Compile config - needed at build and runtime
  compile "org.apache.commons:commons-csv:1.1"
  compile "com.univocity:univocity-parsers:1.5.1"
  compile "org.clapper:grizzled-slf4j_$dependencyScalaVersion:1.0.2"

  forceInclude "org.slf4j:slf4j-api:1.7.16"
  forceInclude "org.slf4j:slf4j-log4j12:1.7.16"

  compile "com.amazonaws:aws-java-sdk:1.3.11"
  hbaseDependency "hbase-client"
  hbaseDependency "hbase-common"
  hbaseDependency "hbase-server"

  // Provided config - needed to build but expected to be available in deploy env
  provided "org.apache.spark:spark-core_$dependencyScalaVersion:$sparkVersion"
  provided "org.apache.spark:spark-yarn_$dependencyScalaVersion:$sparkVersion"
  provided "org.apache.spark:spark-sql_$dependencyScalaVersion:$sparkVersion"
  provided "org.apache.spark:spark-mllib_$dependencyScalaVersion:$sparkVersion"

  testCompile "org.scalatest:scalatest_$dependencyScalaVersion:2.2.5"
  testRuntime 'org.pegdown:pegdown:1.1.0'
}

// publishing setup - url, username, password need to be stored in environment
// variables
publishing {
  publications {
    maven(MavenPublication) {
      from components.java
      artifact docsJar
      artifact sourcesJar
      artifact testJar
    }
  }
}
publishing {
  repositories {
    maven {
      if (version.endsWith("SNAPSHOT")) {
        url "${System.env.MAVEN_REPO_URL}/snapshots"
      } else {
        url "${System.env.MAVEN_REPO_URL}/internal"
      }
      credentials {
        username "${System.env.MAVEN_REPO_USERNAME}"
        password "${System.env.MAVEN_REPO_PASSWORD}"
      }
    }
  }
}

publish << {
  if (System.env.MAVEN_REPO_URL == null) {
    throw new GradleException("Can't publish - missing MAVEN_REPO_URL environment variable")
  }
  if (System.env.MAVEN_REPO_USERNAME == null) {
    throw new GradleException("Can't publish - missing MAVEN_REPO_USERNAME environment variable")
  }
  if (System.env.MAVEN_REPO_PASSWORD == null) {
    throw new GradleException("Can't publish - missing MAVEN_REPO_PASSWORD environment variable")
  }
}

// Creation of a distribution archive containing the pipeline assembly jar and the baseline
// execution scripts.
distributions {
  main {
    baseName = 'xdata-graph'
    contents {
      // Copy the run scripts over
      into ("scripts"){
        from("src/scripts")
        fileMode 0755
      }

      // Copy assembly jar, stripping off the version info.
      into("lib") {
        from configurations.dist.artifacts.files.filter {
          it.name =~ /assembly\w*\.jar/
        }
        rename "(.*)-$version-assembly(.*)", "\$1\$2"
      }
    }
  }
}
